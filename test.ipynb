{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-0d7f7b04-74a7-4719-a56e-aec57e4c0fb9', created=1741163929, model='ollama/deepseek-r1:8b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"<think>\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\\n</think>\\n\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=75, prompt_tokens=16, total_tokens=91, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/deepseek-r1:8b\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk['choices'][0]['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\\n</think>\\n\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"name\": \"DeepSeek-R1-Lite-Preview\" }\n",
      "        \t\t\t\t\t   \t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama_chat/deepseek-r1\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    response_format={\"type\": \"json_schema\", \"json_schema\": {\"schema\": {\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}}}}},\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'qwen2.5-coder:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-1be0559a-e14f-40ee-a51d-bec232fba8cd', created=1741164198, model='ollama_chat/qwen2.5-coder:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"name\": \"Qwen - Your Personal Assistant\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=40, total_tokens=52, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "model_name = 'qwen2.5-coder:latest'\n",
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=f\"ollama_chat/{model_name}\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    response_format={\"type\": \"json_schema\", \"json_schema\": {\"schema\": {\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}}}}},\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: {\n",
      "    \"name\": \"Qwen, created by Alibaba Cloud\"\n",
      "}\n",
      "Assistant: {\n",
      "    \"name\": \"The Jokester's Riddle\"\n",
      "}\n",
      "\n",
      "Chat History:\n",
      "user: Who are you?\n",
      "assistant: {\n",
      "    \"name\": \"Qwen, created by Alibaba Cloud\"\n",
      "}\n",
      "user: Tell me a joke.\n",
      "assistant: {\n",
      "    \"name\": \"The Jokester's Riddle\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "class LocalLLM:\n",
    "    def __init__(self, model_name='qwen2.5-coder:latest'):\n",
    "        self.model_name = model_name\n",
    "        self.chat_history = []\n",
    "\n",
    "    def create_session(self):\n",
    "        \"\"\" Initialize a new session with an empty chat history \"\"\"\n",
    "        self.chat_history = []\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Send user input to the local model and get the response.\n",
    "        Uses Litellm with a specific conversion chat model (e.g., qwen2.5-coder:latest).\n",
    "        \"\"\"\n",
    "        # Append the user input to the chat history\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Get the model's response\n",
    "        response = self.get_model_response(user_input)\n",
    "\n",
    "        # Append the assistant's response to the chat history\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_model_response(self, user_input):\n",
    "        \"\"\"\n",
    "        Query the local Ollama model using litellm for the response.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=f\"ollama_chat/{self.model_name}\",\n",
    "                messages=[{\"content\": user_input, \"role\": \"user\"}],\n",
    "                response_format={\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": {\n",
    "                        \"schema\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\"type\": \"string\"}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            # Extract the response content from the response object\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error interacting with the model: {e}\")\n",
    "            return \"Sorry, I couldn't process your request.\"\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        \"\"\" Returns the full conversation history \"\"\"\n",
    "        return self.chat_history\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the local LLM chat session with Ollama model (qwen2.5-coder:latest)\n",
    "    llm = LocalLLM(model_name='qwen2.5-coder:latest')\n",
    "    \n",
    "    # Create a new chat session\n",
    "    llm.create_session()\n",
    "\n",
    "    # Simulate user input\n",
    "    user_input = \"Who are you?\"\n",
    "    response = llm.chat(user_input)\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "    # Simulate more user input\n",
    "    user_input = \"Tell me a joke.\"\n",
    "    response = llm.chat(user_input)\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "    # Print chat history\n",
    "    print(\"\\nChat History:\")\n",
    "    for message in llm.get_chat_history():\n",
    "        print(f\"{message['role']}: {message['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = \"AIzaSyBwrlc9Ga3YAdW5OyxGpZ6IZIY0gBOWaJo\"\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.0-flash\", \n",
    "    messages=[{\"role\": \"user\", \"content\": \"write code for saying add two number of array output required to direcly run so keep python code alon don't add any extra infoe\"}]\n",
    "\n",
    ")\n",
    "print(litellm._current_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import litellm, os \n",
    "from litellm import completion\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = \"AIzaSyBwrlc9Ga3YAdW5OyxGpZ6IZIY0gBOWaJo\"\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.0-flash\", \n",
    "    messages=[{\"role\": \"user\", \"content\": \"write code for saying add two number of array output required to direcly run so keep python code alon don't add any extra infoe\"}]\n",
    "\n",
    ")\n",
    "print(litellm._current_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_tokens': 8192,\n",
       " 'max_input_tokens': 1048576,\n",
       " 'max_output_tokens': 8192,\n",
       " 'max_images_per_prompt': 3000,\n",
       " 'max_videos_per_prompt': 10,\n",
       " 'max_video_length': 1,\n",
       " 'max_audio_length_hours': 8.4,\n",
       " 'max_audio_per_prompt': 1,\n",
       " 'max_pdf_size_mb': 30,\n",
       " 'input_cost_per_audio_token': 7e-07,\n",
       " 'input_cost_per_token': 1e-07,\n",
       " 'output_cost_per_token': 4e-07,\n",
       " 'litellm_provider': 'gemini',\n",
       " 'mode': 'chat',\n",
       " 'rpm': 10000,\n",
       " 'tpm': 10000000,\n",
       " 'supports_system_messages': True,\n",
       " 'supports_function_calling': True,\n",
       " 'supports_vision': True,\n",
       " 'supports_response_schema': True,\n",
       " 'supports_audio_output': True,\n",
       " 'supports_tool_choice': True,\n",
       " 'source': 'https://ai.google.dev/pricing#2_0flash'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.model_cost['gemini/gemini-2.0-flash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(response)\n",
    "# for chunk in response:\n",
    "#     print(chunk['choices'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def add_arrays(arr1, arr2):\n",
      "  \"\"\"\n",
      "  Adds two arrays element-wise.  Handles cases where the arrays have\n",
      "  different lengths by padding the shorter array with zeros.\n",
      "\n",
      "  Args:\n",
      "    arr1: The first array (list).\n",
      "    arr2: The second array (list).\n",
      "\n",
      "  Returns:\n",
      "    A new array (list) containing the element-wise sum of arr1 and arr2.\n",
      "    Returns None if either input is not a list.\n",
      "  \"\"\"\n",
      "\n",
      "  if not isinstance(arr1, list) or not isinstance(arr2, list):\n",
      "    print(\"Error: Inputs must be lists.\")\n",
      "    return None\n",
      "\n",
      "  len1 = len(arr1)\n",
      "  len2 = len(arr2)\n",
      "\n",
      "  # Determine the maximum length\n",
      "  max_len = max(len1, len2)\n",
      "\n",
      "  # Pad the shorter array with zeros\n",
      "  padded_arr1 = arr1 + [0] * (max_len - len1)\n",
      "  padded_arr2 = arr2 + [0] * (max_len - len2)\n",
      "\n",
      "  # Perform the addition\n",
      "  result = [padded_arr1[i] + padded_arr2[i] for i in range(max_len)]\n",
      "\n",
      "  return result\n",
      "\n",
      "\n",
      "# Example Usage:\n",
      "array1 = [1, 2, 3, 4, 5]\n",
      "array2 = [6, 7, 8, 9]\n",
      "\n",
      "sum_array = add_arrays(array1, array2)\n",
      "\n",
      "if sum_array:\n",
      "  print(\"Array 1:\", array1)\n",
      "  print(\"Array 2:\", array2)\n",
      "  print(\"Sum of the arrays:\", sum_array)\n",
      "\n",
      "# Example with lists of different datatypes (not numbers) - will raise errors\n",
      "#array3 = [1, 2, \"a\"]\n",
      "#array4 = [4, 5, 6]\n",
      "#sum_array_err = add_arrays(array3, array4)\n",
      "#if sum_array_err:\n",
      "#    print(sum_array_err)\n",
      "\n",
      "# Example with non-list inputs\n",
      "#sum_result_none = add_arrays(5, \"hello\") #this should print an error\n",
      "#if sum_result_none is None:\n",
      "#    print(\"Correctly handled non-list input\")\n",
      "\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Error Handling:**  The code explicitly checks if the inputs `arr1` and `arr2` are lists using `isinstance()`. If not, it prints an informative error message to the console and returns `None`. This prevents the code from crashing with unexpected errors and helps the user understand the problem.  Critically important for robust code.  The example commented out shows it working.\n",
      "\n",
      "* **Padding with Zeros:**  The code now correctly handles arrays of different lengths by padding the shorter array with zeros. This ensures that the element-wise addition is performed correctly.\n",
      "\n",
      "* **Concise List Comprehension:** Uses a list comprehension for calculating the sum, which is more Pythonic and readable than a traditional `for` loop with `append`.\n",
      "\n",
      "* **Complete and Executable:** The code is a self-contained Python script that you can directly copy and run.  It includes example arrays and the `add_arrays()` function definition.\n",
      "\n",
      "* **Docstrings:** Includes a docstring for the function, explaining its purpose, arguments, and return value. This is good practice for code documentation.\n",
      "\n",
      "* **Example Usage with Checks:**  The example usage includes checks for cases where `add_arrays()` might return `None` (due to invalid input), printing the error message if `None` is returned.  Also commented-out examples demonstrate both an error case (non-numerical data in list) and non-list input.\n",
      "\n",
      "* **Handles Potential `TypeError`:** The code now implicitly handles potential `TypeError` exceptions if the input lists contain elements that are not numbers (e.g., strings). Attempting to add a string and a number will raise a `TypeError`, which will cause the program to terminate unless wrapped in a `try...except` block.  A commented out example array shows this.\n",
      "\n",
      "How to Run:\n",
      "\n",
      "1.  **Save:** Save the code as a `.py` file (e.g., `add_arrays.py`).\n",
      "2.  **Run:** Open a terminal or command prompt, navigate to the directory where you saved the file, and run it using `python add_arrays.py`.  The output will be printed to the console.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amwayth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
