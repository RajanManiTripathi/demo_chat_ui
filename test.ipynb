{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-0d7f7b04-74a7-4719-a56e-aec57e4c0fb9', created=1741163929, model='ollama/deepseek-r1:8b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"<think>\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\\n</think>\\n\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=75, prompt_tokens=16, total_tokens=91, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/deepseek-r1:8b\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk['choices'][0]['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\\n</think>\\n\\nI'm DeepSeek-R1, an AI assistant created exclusively by the Chinese Company DeepSeek. I specialize in helping you tackle complex STEM challenges through analytical thinking and logical reasoning.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"name\": \"DeepSeek-R1-Lite-Preview\" }\n",
      "        \t\t\t\t\t   \t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama_chat/deepseek-r1\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    response_format={\"type\": \"json_schema\", \"json_schema\": {\"schema\": {\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}}}}},\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'qwen2.5-coder:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-1be0559a-e14f-40ee-a51d-bec232fba8cd', created=1741164198, model='ollama_chat/qwen2.5-coder:latest', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"name\": \"Qwen - Your Personal Assistant\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=40, total_tokens=52, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "model_name = 'qwen2.5-coder:latest'\n",
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=f\"ollama_chat/{model_name}\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    response_format={\"type\": \"json_schema\", \"json_schema\": {\"schema\": {\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}}}}},\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: {\n",
      "    \"name\": \"Qwen, created by Alibaba Cloud\"\n",
      "}\n",
      "Assistant: {\n",
      "    \"name\": \"The Jokester's Riddle\"\n",
      "}\n",
      "\n",
      "Chat History:\n",
      "user: Who are you?\n",
      "assistant: {\n",
      "    \"name\": \"Qwen, created by Alibaba Cloud\"\n",
      "}\n",
      "user: Tell me a joke.\n",
      "assistant: {\n",
      "    \"name\": \"The Jokester's Riddle\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "class LocalLLM:\n",
    "    def __init__(self, model_name='qwen2.5-coder:latest'):\n",
    "        self.model_name = model_name\n",
    "        self.chat_history = []\n",
    "\n",
    "    def create_session(self):\n",
    "        \"\"\" Initialize a new session with an empty chat history \"\"\"\n",
    "        self.chat_history = []\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Send user input to the local model and get the response.\n",
    "        Uses Litellm with a specific conversion chat model (e.g., qwen2.5-coder:latest).\n",
    "        \"\"\"\n",
    "        # Append the user input to the chat history\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Get the model's response\n",
    "        response = self.get_model_response(user_input)\n",
    "\n",
    "        # Append the assistant's response to the chat history\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_model_response(self, user_input):\n",
    "        \"\"\"\n",
    "        Query the local Ollama model using litellm for the response.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=f\"ollama_chat/{self.model_name}\",\n",
    "                messages=[{\"content\": user_input, \"role\": \"user\"}],\n",
    "                response_format={\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": {\n",
    "                        \"schema\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\"type\": \"string\"}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            # Extract the response content from the response object\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error interacting with the model: {e}\")\n",
    "            return \"Sorry, I couldn't process your request.\"\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        \"\"\" Returns the full conversation history \"\"\"\n",
    "        return self.chat_history\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the local LLM chat session with Ollama model (qwen2.5-coder:latest)\n",
    "    llm = LocalLLM(model_name='qwen2.5-coder:latest')\n",
    "    \n",
    "    # Create a new chat session\n",
    "    llm.create_session()\n",
    "\n",
    "    # Simulate user input\n",
    "    user_input = \"Who are you?\"\n",
    "    response = llm.chat(user_input)\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "    # Simulate more user input\n",
    "    user_input = \"Tell me a joke.\"\n",
    "    response = llm.chat(user_input)\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "    # Print chat history\n",
    "    print(\"\\nChat History:\")\n",
    "    for message in llm.get_chat_history():\n",
    "        print(f\"{message['role']}: {message['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = \"AIzaSyBwrlc9Ga3YAdW5OyxGpZ6IZIY0gBOWaJo\"\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.0-flash\", \n",
    "    messages=[{\"role\": \"user\", \"content\": \"write code for saying hi from LiteLLM output required to direcly run so keep python code alone\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()  # Load environment variables from .env file\n",
      "\n",
      "import litellm\n",
      "\n",
      "# Set your OpenAI API key (or other provider's key)\n",
      "litellm.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "# Example using OpenAI (you can change model to another provider and model)\n",
      "response = litellm.completion(\n",
      "    model=\"gpt-3.5-turbo\",\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Say hi.\"}],\n",
      ")\n",
      "\n",
      "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amwayth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
